{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: Analyzing Stock Sentiment from Twits\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. You want to be able to turn this information into a signal. Currently, the best tool for the job is a Neural Network. \n",
    "\n",
    "For this project, you'll use posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits). The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is called a Twit. This is similar to Twitter's version of a post, called a Tweet. You'll build a model around these twits that generate a sentiment score.\n",
    "\n",
    "We've collected a bunch of twits, then hand labeled the sentiment of each. To capture the degree of sentiment, we'll use a five-point scale: very negative, negative, neutral, positive, very positive. Each twit is labeled -2 to 2 in steps of 1, from very negative to very positive respectively. You'll build a sentiment analysis model that will learn to assign sentiment to twits on its own, using this labeled data.\n",
    "\n",
    "The first thing we should to do, is load the data.\n",
    "\n",
    "## Import Twits \n",
    "### Load Twits Data \n",
    "This JSON file contains a list of objects for each twit in the `'data'` field:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Neutral twit body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy twit body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```\n",
    "\n",
    "The fields represent the following:\n",
    "\n",
    "* `'message_body'`: The text of the twit.\n",
    "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n",
    "\n",
    "\n",
    "To see what the data look like by printing the first 10 twits from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
     ]
    }
   ],
   "source": [
    "#with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
    "with open('twits.json', 'r') as f:\n",
    "    twits = json.load(f)\n",
    "\n",
    "print(twits['data'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message_body': '$FITB great buy at 26.00...ill wait',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2018-07-01T00:00:09Z'},\n",
       " {'message_body': '@StockTwits $MSFT',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2018-07-01T00:00:42Z'},\n",
       " {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2018-07-01T00:01:24Z'},\n",
       " {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2018-07-01T00:01:47Z'},\n",
       " {'message_body': '$AMD reveal yourself!',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2018-07-01T00:02:13Z'},\n",
       " {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2018-07-01T00:03:10Z'},\n",
       " {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA',\n",
       "  'sentiment': -2,\n",
       "  'timestamp': '2018-07-01T00:04:09Z'},\n",
       " {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2018-07-01T00:04:17Z'},\n",
       " {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2018-07-01T00:08:01Z'},\n",
       " {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?',\n",
       "  'sentiment': -2,\n",
       "  'timestamp': '2018-07-01T00:09:03Z'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twits['data'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of Data\n",
    "Now let's look at the number of twits in dataset. Print the number of twits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548010\n"
     ]
    }
   ],
   "source": [
    "\"\"\"print out the number of twits\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "print(len(twits['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Message Body and Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [twit['message_body'] for twit in twits['data']]\n",
    "# Since the sentiment scores are discrete, \n",
    "# we'll scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "With our data in hand we need to preprocess our text. These twits are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the twit itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "**The ticker symbols don't provide information on the sentiment, and they are in every twit, so we should remove them.** \n",
    "\n",
    "**This twit also has the `@google` username, again not providing sentiment information, so we should also remove it.**\n",
    "\n",
    "**We also see a URL `http://t.co/sptHOAh8`. Let's remove these too.**\n",
    "\n",
    "The easiest way to remove specific words or phrases is with regex using the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sealbasket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - tokenize by splitting the string on whitespace \n",
    "        - removes any single character tokens\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        tokens: The preprocessed text into tokens.\n",
    "    \"\"\" \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    \n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub(r'(http|https):/(/[a-zA-Z0-9\\.]+)+', ' ', text)\n",
    "    \n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub(r'\\$[A-Z]+', ' ', text) \n",
    "    \n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub(r'\\@[a-zA-Z0-9]+', ' ',text)\n",
    "\n",
    "    # Replace everything not a letter with a space\n",
    "    text = re.sub(r'\\W\\d', ' ', text)\n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace into a list of words\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Lemmatize words using the WordNetLemmatizer. \n",
    "    # You can ignore any word that is not longer than one character.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [wnl.lemmatize(w) for w in tokens if len(w)>=2]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. Apply the function `preprocess` to all the twit messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement\n",
    "\n",
    "tokenized = [preprocess(msg) for msg in messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a vocabulary by using Bag of words\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Implement \n",
    "bow = Counter()\n",
    "for msg in tokenized:\n",
    "    bow.update(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19196624"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bow.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of Words Appearing in Message\n",
    "With our vocabulary, now we'll **remove some of the most common words such as 'the', 'and', 'it', etc.** These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning.\n",
    "\n",
    "We also want to **remove really rare words that show up in a only a few twits.** Here you'll want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'for', 'of', 'is', '$amd', 'on', '$aapl', 'the', 'to', 'in', 'and', '$amzn', 'this', 'it', '$mu', 'will', '9;s', '$fb', 'are', 'up']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    freqs\n",
    "    low_cutoff\n",
    "    high_cutoff\n",
    "    K_most_common\n",
    "\"\"\"\n",
    "\n",
    "# TODO Implement \n",
    "\n",
    "# Dictionary that contains the Frequency of words appearing in messages.\n",
    "# The key is the token and the value is the frequency of that word in the corpus.\n",
    "N = sum(bow.values())\n",
    "freqs = {key: bow[key]/N for key in bow} \n",
    "\n",
    "# Float that is the frequency cutoff. \n",
    "# Drop words with a frequency that is lower or equal to this number.\n",
    "low_cutoff = 2.0e-06\n",
    "\n",
    "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
    "high_cutoff = 0.005\n",
    "\n",
    "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
    "K_most_common = [word for word in freqs if freqs[word]>high_cutoff]\n",
    "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
    "print(K_most_common)\n",
    "len(filtered_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Vocabulary by Removing Filtered Words\n",
    "Let's creat three variables that will help with our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$fitb', 'great', 'buy', 'at', '0...ill', 'wait']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set the following variables:\n",
    "    vocab\n",
    "    id2vocab\n",
    "    filtered\n",
    "\"\"\"\n",
    "\n",
    "#TODO Implement\n",
    "\n",
    "# A dictionary for the `filtered_words`. \n",
    "# The key is the word and value is an id that represents the word. \n",
    "vocab = {word : i for i, word in enumerate(filtered_words, 1)} # 1부터 enum해야함\n",
    "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
    "id2vocab = {vocab[word]: word for word in vocab}\n",
    "# tokenized data with the words not in `filtered_words` removed.\n",
    "filtered = [[w for w in words if w in vocab] for words in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "Let's do a few last pre-processing steps. If we look at how our twits are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes.\n",
    "That is, make sure each of our different sentiment scores show up roughly as frequently in the data.\n",
    "\n",
    "What we can do here is go through each of our examples and randomly drop twits with neutral sentiment. What should be the probability we drop these twits if we want to **get around 20% neutral twits starting at 50% neutral?** We should also take this opportunity to **remove messages with length 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {'messages': [], 'sentiments':[]}\n",
    "\n",
    "n_neutral = sum(1 for each in sentiments if each == 2) \n",
    "N_examples = len(sentiments) \n",
    "\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral \n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    message = filtered[idx]\n",
    "    if len(message) == 0:\n",
    "        # skip this message because it has length zero\n",
    "        continue\n",
    "    elif sentiment != 2 or random.random() < keep_prob:\n",
    "        balanced['messages'].append(message)\n",
    "        balanced['sentiments'].append(sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19705195076552667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Now we have our vocabulary which means we can transform our tokens into ids, which are then passed to our network. So, let's define the network now!\n",
    "\n",
    "Here is a nice diagram showing the network we'd like to build: \n",
    "\n",
    "#### Embed -> RNN -> Dense -> Softmax\n",
    "### Implement the text classifier\n",
    "Before we build text classifier, if you remember from the other network that you built in  \"Sentiment Analysis with an RNN\"  exercise  - which there, the network called \" SentimentRNN\", here we named it \"TextClassifer\" - consists of three main parts: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "This network is pretty similar to the network you built expect in the  `forward` pass, we use softmax instead of sigmoid. The reason we are not using sigmoid is that the output of NN is not a binary. In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            vocab_size : The vocabulary size.\n",
    "            embed_size : The embedding layer size.\n",
    "            lstm_size : The LSTM layer size.\n",
    "            output_size : The output size.\n",
    "            lstm_layers : The number of LSTM layers.\n",
    "            dropout : The dropout probability.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # TODO Implement\n",
    "\n",
    "        # Setup embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Setup additional layers\n",
    "        # [seq, batch, embed]\n",
    "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Initializes hidden state\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            batch_size : The size of batches.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            hidden_state\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement \n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        w = next(self.parameters()).data\n",
    "        \n",
    "        \n",
    "        hidden_state = (w.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "                     w.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "        \n",
    "        return hidden_state\n",
    "\n",
    "\n",
    "    def forward(self, nn_input, hidden_state):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on nn_input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            nn_input : The batch of input to the NN. [seq, batch,vocab]\n",
    "            hidden_state : The LSTM hidden state. ([num_layer, batch, hidden], [num_layer, batch, hidden])\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            logps: log softmax output\n",
    "            hidden_state: The new hidden state.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = nn_input.size(1)\n",
    "        \n",
    "        # TODO Implement\n",
    "        # [seq, batch, embedding]\n",
    "        embed = self.embedding(nn_input)\n",
    "        # lstm_out : [seq, batch, hidden_size] \n",
    "        lstm_out, hidden_state = self.lstm(embed, hidden_state) # <--\n",
    "        \n",
    "        # [batch, hidden_size]\n",
    "        last_out = lstm_out[-1,:,:]\n",
    "        \n",
    "        # out : [batch, output_size]\n",
    "        out = self.fc(last_out)\n",
    "        logps = self.softmax(out)\n",
    "\n",
    "\n",
    "        return logps, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7383, -1.6581, -1.4826, -1.8780, -1.3714],\n",
      "        [-1.7156, -1.6774, -1.4744, -1.8408, -1.4038],\n",
      "        [-1.7510, -1.6448, -1.5083, -1.8797, -1.3493],\n",
      "        [-1.7101, -1.6823, -1.4681, -1.8330, -1.4151]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### DataLoaders and Batching\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \"\"\" \n",
    "    Build a dataloader.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        indices = list(range(len(messages)))\n",
    "        random.shuffle(indices)\n",
    "        messages = [messages[idx] for idx in indices]\n",
    "        labels = [labels[idx] for idx in indices]\n",
    "\n",
    "    total_sequences = len(messages)\n",
    "\n",
    "    for ii in range(0, total_sequences, batch_size):\n",
    "        batch_messages = messages[ii: ii+batch_size]\n",
    "        \n",
    "        # First initialize a tensor of all zeros\n",
    "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
    "        for batch_num, tokens in enumerate(batch_messages):\n",
    "            token_tensor = torch.tensor(tokens)\n",
    "            # Left pad!\n",
    "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
    "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
    "        \n",
    "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
    "        \n",
    "        yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data into training and validation datasets. Use an appropriate split size.\n",
    "The features are the `token_ids` and the labels are the `sentiments`.\n",
    "\"\"\"   \n",
    "\n",
    "# TODO Implement \n",
    "frac = 0.9\n",
    "last_train_idx = int(len(token_ids)*frac)\n",
    "train_features = token_ids[:last_train_idx]\n",
    "valid_features = token_ids[last_train_idx:]\n",
    "train_labels = sentiments[:last_train_idx]\n",
    "valid_labels = sentiments[last_train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "It's time to train the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(19171, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch: 1/2 Step : 100 Train Loss : 1.449468 Validation error 1.5295671915291718\n",
      "Epoch: 1/2 Step : 200 Train Loss : 1.319110 Validation error 1.3546472064269093\n",
      "Epoch: 1/2 Step : 300 Train Loss : 1.091652 Validation error 1.2939295514020213\n",
      "Epoch: 1/2 Step : 400 Train Loss : 1.194512 Validation error 1.267043459824738\n",
      "Epoch: 1/2 Step : 500 Train Loss : 1.159561 Validation error 1.2527397995635383\n",
      "Epoch: 1/2 Step : 600 Train Loss : 1.112111 Validation error 1.216643022730773\n",
      "Epoch: 1/2 Step : 700 Train Loss : 1.210455 Validation error 1.199769615213393\n",
      "Epoch: 1/2 Step : 800 Train Loss : 1.100383 Validation error 1.1974871919829009\n",
      "Epoch: 1/2 Step : 900 Train Loss : 1.286013 Validation error 1.1883463159724292\n",
      "Epoch: 1/2 Step : 1000 Train Loss : 0.974574 Validation error 1.168887387383284\n",
      "Epoch: 1/2 Step : 1100 Train Loss : 1.181630 Validation error 1.1602657757091932\n",
      "Epoch: 1/2 Step : 1200 Train Loss : 1.025319 Validation error 1.1461382583194828\n",
      "Epoch: 1/2 Step : 1300 Train Loss : 1.155370 Validation error 1.1334241973264754\n",
      "Epoch: 1/2 Step : 1400 Train Loss : 1.005533 Validation error 1.1297432556245899\n",
      "Epoch: 1/2 Step : 1500 Train Loss : 0.861666 Validation error 1.128144721653036\n",
      "Epoch: 1/2 Step : 1600 Train Loss : 1.017373 Validation error 1.1162393103341688\n",
      "Epoch: 1/2 Step : 1700 Train Loss : 1.020538 Validation error 1.1015224712235587\n",
      "Epoch: 1/2 Step : 1800 Train Loss : 0.959772 Validation error 1.0938910867888674\n",
      "Epoch: 1/2 Step : 1900 Train Loss : 0.923904 Validation error 1.0844934194969444\n",
      "Epoch: 1/2 Step : 2000 Train Loss : 1.018237 Validation error 1.0875170705217587\n",
      "Epoch: 1/2 Step : 2100 Train Loss : 1.099552 Validation error 1.0720315676097998\n",
      "Epoch: 1/2 Step : 2200 Train Loss : 1.147279 Validation error 1.0760039616625123\n",
      "Epoch: 1/2 Step : 2300 Train Loss : 1.189320 Validation error 1.0712766453651181\n",
      "Epoch: 1/2 Step : 2400 Train Loss : 0.850333 Validation error 1.0609494635183192\n",
      "Epoch: 1/2 Step : 2500 Train Loss : 1.260303 Validation error 1.0642482847761041\n",
      "Epoch: 1/2 Step : 2600 Train Loss : 0.918491 Validation error 1.0495738664848684\n",
      "Epoch: 1/2 Step : 2700 Train Loss : 0.897628 Validation error 1.0456920050171385\n",
      "Epoch: 1/2 Step : 2800 Train Loss : 1.029540 Validation error 1.0577735651277604\n",
      "Epoch: 1/2 Step : 2900 Train Loss : 0.948819 Validation error 1.0516756905652789\n",
      "Epoch: 1/2 Step : 3000 Train Loss : 0.965179 Validation error 1.0429524735831395\n",
      "Epoch: 1/2 Step : 3100 Train Loss : 0.853140 Validation error 1.0324819807184757\n",
      "Epoch: 1/2 Step : 3200 Train Loss : 1.143527 Validation error 1.0366394747480303\n",
      "Epoch: 1/2 Step : 3300 Train Loss : 1.081490 Validation error 1.025476625183475\n",
      "Epoch: 1/2 Step : 3400 Train Loss : 0.928775 Validation error 1.0205322319857553\n",
      "Epoch: 1/2 Step : 3500 Train Loss : 1.033572 Validation error 1.0149815950855603\n",
      "Epoch: 1/2 Step : 3600 Train Loss : 0.872281 Validation error 1.0122522625873456\n",
      "Epoch: 1/2 Step : 3700 Train Loss : 0.954691 Validation error 1.0056901422473714\n",
      "Epoch: 1/2 Step : 3800 Train Loss : 0.947373 Validation error 1.0102828526920808\n",
      "Epoch: 1/2 Step : 3900 Train Loss : 0.915808 Validation error 1.0077014351680482\n",
      "Epoch: 1/2 Step : 4000 Train Loss : 0.892327 Validation error 0.9984506287872974\n",
      "Epoch: 1/2 Step : 4100 Train Loss : 0.823849 Validation error 0.9956805211572396\n",
      "Epoch: 1/2 Step : 4200 Train Loss : 0.874434 Validation error 1.0199626128341726\n",
      "Epoch: 1/2 Step : 4300 Train Loss : 0.894847 Validation error 0.9901707383257563\n",
      "Epoch: 1/2 Step : 4400 Train Loss : 1.098530 Validation error 0.9873911630588393\n",
      "Epoch: 1/2 Step : 4500 Train Loss : 0.871420 Validation error 0.9889758286631085\n",
      "Epoch: 1/2 Step : 4600 Train Loss : 0.901790 Validation error 0.9937213302243348\n",
      "Epoch: 1/2 Step : 4700 Train Loss : 0.788435 Validation error 0.9838320187525396\n",
      "Epoch: 1/2 Step : 4800 Train Loss : 1.035164 Validation error 0.9798939589120945\n",
      "Epoch: 1/2 Step : 4900 Train Loss : 0.967093 Validation error 0.973998234507352\n",
      "Epoch: 1/2 Step : 5000 Train Loss : 0.860192 Validation error 0.9710917923071866\n",
      "Epoch: 1/2 Step : 5100 Train Loss : 1.014993 Validation error 0.9791710473217897\n",
      "Epoch: 1/2 Step : 5200 Train Loss : 0.906239 Validation error 0.9623529101938356\n",
      "Epoch: 1/2 Step : 5300 Train Loss : 0.925688 Validation error 0.9685665352954081\n",
      "Epoch: 1/2 Step : 5400 Train Loss : 0.924850 Validation error 0.963991702041474\n",
      "Epoch: 1/2 Step : 5500 Train Loss : 1.036891 Validation error 0.971768336271231\n",
      "Epoch: 1/2 Step : 5600 Train Loss : 0.911687 Validation error 0.9614474980616262\n",
      "Epoch: 1/2 Step : 5700 Train Loss : 0.932655 Validation error 0.9578313196385148\n",
      "Epoch: 1/2 Step : 5800 Train Loss : 0.892870 Validation error 0.9537830198198503\n",
      "Epoch: 1/2 Step : 5900 Train Loss : 0.895466 Validation error 0.9534247393333276\n",
      "Epoch: 1/2 Step : 6000 Train Loss : 0.941709 Validation error 0.9516949912363002\n",
      "Epoch: 1/2 Step : 6100 Train Loss : 0.843281 Validation error 0.9510065164031023\n",
      "Epoch: 1/2 Step : 6200 Train Loss : 0.851326 Validation error 0.9491513446749974\n",
      "Epoch: 1/2 Step : 6300 Train Loss : 0.806550 Validation error 0.9429679662080575\n",
      "Epoch: 1/2 Step : 6400 Train Loss : 0.922205 Validation error 0.9445778048893632\n",
      "Epoch: 1/2 Step : 6500 Train Loss : 0.811053 Validation error 0.951911330113449\n",
      "Epoch: 1/2 Step : 6600 Train Loss : 0.834503 Validation error 0.9411091452212074\n",
      "Epoch: 1/2 Step : 6700 Train Loss : 0.840900 Validation error 0.9392644710002831\n",
      "Epoch: 1/2 Step : 6800 Train Loss : 0.734748 Validation error 0.9348111116834976\n",
      "Epoch: 1/2 Step : 6900 Train Loss : 0.954706 Validation error 0.9398855523490087\n",
      "Epoch: 1/2 Step : 7000 Train Loss : 0.734504 Validation error 0.9427879780878106\n",
      "Epoch: 1/2 Step : 7100 Train Loss : 0.994076 Validation error 0.9320721056913028\n",
      "Epoch: 1/2 Step : 7200 Train Loss : 0.810473 Validation error 0.9326071266671911\n",
      "Epoch: 1/2 Step : 7300 Train Loss : 0.885801 Validation error 0.926980315323332\n",
      "Epoch: 1/2 Step : 7400 Train Loss : 0.829260 Validation error 0.9245410755249271\n",
      "Epoch: 1/2 Step : 7500 Train Loss : 0.816864 Validation error 0.9305276439498349\n",
      "Epoch: 1/2 Step : 7600 Train Loss : 0.906758 Validation error 0.9254033089341889\n",
      "Epoch: 1/2 Step : 7700 Train Loss : 0.803818 Validation error 0.9277338392525492\n",
      "Epoch: 1/2 Step : 7800 Train Loss : 0.885680 Validation error 0.9325943704692226\n",
      "Epoch: 1/2 Step : 7900 Train Loss : 1.091533 Validation error 0.922620793963268\n",
      "Epoch: 1/2 Step : 8000 Train Loss : 0.929626 Validation error 0.933300683828748\n",
      "Epoch: 1/2 Step : 8100 Train Loss : 0.924199 Validation error 0.9190156812831568\n",
      "Epoch: 1/2 Step : 8200 Train Loss : 0.871645 Validation error 0.9147772125347051\n",
      "Epoch: 1/2 Step : 8300 Train Loss : 0.922049 Validation error 0.9113372572355662\n",
      "Epoch: 1/2 Step : 8400 Train Loss : 0.950193 Validation error 0.907857509239695\n",
      "Epoch: 1/2 Step : 8500 Train Loss : 0.852790 Validation error 0.9104640869038885\n",
      "Epoch: 1/2 Step : 8600 Train Loss : 0.945128 Validation error 0.9098452781329924\n",
      "Epoch: 1/2 Step : 8700 Train Loss : 0.889542 Validation error 0.9123705689487539\n",
      "Epoch: 1/2 Step : 8800 Train Loss : 0.763817 Validation error 0.90650064122231\n",
      "Epoch: 1/2 Step : 8900 Train Loss : 1.027127 Validation error 0.9048825170933872\n",
      "Epoch: 1/2 Step : 9000 Train Loss : 0.901923 Validation error 0.9007147870350294\n",
      "Epoch: 1/2 Step : 9100 Train Loss : 1.043828 Validation error 0.9070583351691905\n",
      "Epoch: 1/2 Step : 9200 Train Loss : 1.070395 Validation error 0.9033256086522564\n",
      "Epoch: 1/2 Step : 9300 Train Loss : 0.846584 Validation error 0.8957153806322414\n",
      "Epoch: 1/2 Step : 9400 Train Loss : 0.822009 Validation error 0.8998876944924337\n",
      "Epoch: 1/2 Step : 9500 Train Loss : 0.892565 Validation error 0.8954668471156857\n",
      "Epoch: 1/2 Step : 9600 Train Loss : 0.845397 Validation error 0.8926974773626252\n",
      "Epoch: 1/2 Step : 9700 Train Loss : 0.810783 Validation error 0.9007792502075233\n",
      "Epoch: 1/2 Step : 9800 Train Loss : 0.998880 Validation error 0.9013651212543743\n",
      "Epoch: 1/2 Step : 9900 Train Loss : 0.773369 Validation error 0.8917642134526695\n",
      "Epoch: 1/2 Step : 10000 Train Loss : 0.876168 Validation error 0.8964475728411532\n",
      "Epoch: 1/2 Step : 10100 Train Loss : 1.089326 Validation error 0.8967986737271162\n",
      "Epoch: 1/2 Step : 10200 Train Loss : 0.847204 Validation error 0.8941275559635268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Step : 10300 Train Loss : 0.819852 Validation error 0.8937994413475256\n",
      "Epoch: 1/2 Step : 10400 Train Loss : 0.968822 Validation error 0.8861185573201322\n",
      "Epoch: 1/2 Step : 10500 Train Loss : 0.788646 Validation error 0.8849260003050728\n",
      "Epoch: 1/2 Step : 10600 Train Loss : 0.835511 Validation error 0.8848859378160105\n",
      "Epoch: 1/2 Step : 10700 Train Loss : 0.738123 Validation error 0.883234672572699\n",
      "Epoch: 1/2 Step : 10800 Train Loss : 0.930928 Validation error 0.8825467649886098\n",
      "Epoch: 1/2 Step : 10900 Train Loss : 0.696212 Validation error 0.8831019805736297\n",
      "Epoch: 1/2 Step : 11000 Train Loss : 0.695045 Validation error 0.8802101176476493\n",
      "Epoch: 1/2 Step : 11100 Train Loss : 0.817043 Validation error 0.8851306605455994\n",
      "Epoch: 1/2 Step : 11200 Train Loss : 0.791702 Validation error 0.8755607659432291\n",
      "Epoch: 1/2 Step : 11300 Train Loss : 0.688774 Validation error 0.8764230288807269\n",
      "Epoch: 1/2 Step : 11400 Train Loss : 0.781951 Validation error 0.8745985507672729\n",
      "Epoch: 1/2 Step : 11500 Train Loss : 0.810937 Validation error 0.8765677543668788\n",
      "Epoch: 1/2 Step : 11600 Train Loss : 0.924218 Validation error 0.8770535546603341\n",
      "Epoch: 1/2 Step : 11700 Train Loss : 1.007197 Validation error 0.8747684232008962\n",
      "Epoch: 1/2 Step : 11800 Train Loss : 0.653220 Validation error 0.8744664458611638\n",
      "Epoch: 1/2 Step : 11900 Train Loss : 0.907547 Validation error 0.8721275716599769\n",
      "Epoch: 1/2 Step : 12000 Train Loss : 0.884215 Validation error 0.8720481202764646\n",
      "Epoch: 1/2 Step : 12100 Train Loss : 0.998745 Validation error 0.865800683480841\n",
      "Epoch: 1/2 Step : 12200 Train Loss : 0.793633 Validation error 0.8669968447679395\n",
      "Epoch: 1/2 Step : 12300 Train Loss : 0.847625 Validation error 0.872065132012183\n",
      "Epoch: 1/2 Step : 12400 Train Loss : 0.829447 Validation error 0.8666492569380783\n",
      "Epoch: 1/2 Step : 12500 Train Loss : 0.616289 Validation error 0.8641240877070213\n",
      "Epoch: 1/2 Step : 12600 Train Loss : 0.967073 Validation error 0.8638328930704525\n",
      "Epoch: 1/2 Step : 12700 Train Loss : 0.912720 Validation error 0.8735655301703331\n",
      "Epoch: 1/2 Step : 12800 Train Loss : 0.738033 Validation error 0.8688975931091004\n",
      "Epoch: 1/2 Step : 12900 Train Loss : 0.823421 Validation error 0.8659162980731027\n",
      "Epoch: 1/2 Step : 13000 Train Loss : 0.928209 Validation error 0.8633127254550987\n",
      "Epoch: 1/2 Step : 13100 Train Loss : 0.620278 Validation error 0.8613330337037929\n",
      "Epoch: 1/2 Step : 13200 Train Loss : 0.749239 Validation error 0.8643098834246525\n",
      "Epoch: 1/2 Step : 13300 Train Loss : 0.700749 Validation error 0.8590115344210097\n",
      "Epoch: 1/2 Step : 13400 Train Loss : 0.815383 Validation error 0.8616055400184441\n",
      "Epoch: 1/2 Step : 13500 Train Loss : 0.850039 Validation error 0.8615818193615177\n",
      "Epoch: 1/2 Step : 13600 Train Loss : 0.783157 Validation error 0.8628366100941605\n",
      "Epoch: 1/2 Step : 13700 Train Loss : 0.739463 Validation error 0.8573196230491635\n",
      "Epoch: 1/2 Step : 13800 Train Loss : 1.010473 Validation error 0.8608195280860641\n",
      "Epoch: 1/2 Step : 13900 Train Loss : 0.748608 Validation error 0.8585584970941432\n",
      "Epoch: 1/2 Step : 14000 Train Loss : 0.723413 Validation error 0.8647360828374515\n",
      "Epoch: 1/2 Step : 14100 Train Loss : 0.784887 Validation error 0.8595219078347552\n",
      "Epoch: 1/2 Step : 14200 Train Loss : 0.982874 Validation error 0.8526139012295216\n",
      "Epoch: 1/2 Step : 14300 Train Loss : 0.787318 Validation error 0.8554194814072731\n",
      "Epoch: 1/2 Step : 14400 Train Loss : 0.777009 Validation error 0.8541963884320308\n",
      "Epoch: 1/2 Step : 14500 Train Loss : 0.884053 Validation error 0.8564384852648512\n",
      "Epoch: 1/2 Step : 14600 Train Loss : 0.733498 Validation error 0.8501137548030921\n",
      "Starting epoch 2\n",
      "Epoch: 2/2 Step : 100 Train Loss : 0.750326 Validation error 0.8579521877205788\n",
      "Epoch: 2/2 Step : 200 Train Loss : 0.762071 Validation error 0.8492445081454124\n",
      "Epoch: 2/2 Step : 300 Train Loss : 0.715285 Validation error 0.8580599321608599\n",
      "Epoch: 2/2 Step : 400 Train Loss : 0.789684 Validation error 0.8491560427379491\n",
      "Epoch: 2/2 Step : 500 Train Loss : 0.831025 Validation error 0.8500884656230779\n",
      "Epoch: 2/2 Step : 600 Train Loss : 0.832078 Validation error 0.8490867624247754\n",
      "Epoch: 2/2 Step : 700 Train Loss : 0.756430 Validation error 0.8540999913127929\n",
      "Epoch: 2/2 Step : 800 Train Loss : 1.043182 Validation error 0.8501348920134516\n",
      "Epoch: 2/2 Step : 900 Train Loss : 0.659919 Validation error 0.8485362185502184\n",
      "Epoch: 2/2 Step : 1000 Train Loss : 0.636476 Validation error 0.8439093806639835\n",
      "Epoch: 2/2 Step : 1100 Train Loss : 0.831081 Validation error 0.8451447697193033\n",
      "Epoch: 2/2 Step : 1200 Train Loss : 0.805900 Validation error 0.8492648749173163\n",
      "Epoch: 2/2 Step : 1300 Train Loss : 0.524958 Validation error 0.8477104399593092\n",
      "Epoch: 2/2 Step : 1400 Train Loss : 0.651520 Validation error 0.8464700149327681\n",
      "Epoch: 2/2 Step : 1500 Train Loss : 0.677661 Validation error 0.8472465053063678\n",
      "Epoch: 2/2 Step : 1600 Train Loss : 0.715438 Validation error 0.8496671376748731\n",
      "Epoch: 2/2 Step : 1700 Train Loss : 0.560692 Validation error 0.8468173133091719\n",
      "Epoch: 2/2 Step : 1800 Train Loss : 0.766078 Validation error 0.8442429925202592\n",
      "Epoch: 2/2 Step : 1900 Train Loss : 0.930264 Validation error 0.8464099464059827\n",
      "Epoch: 2/2 Step : 2000 Train Loss : 0.834481 Validation error 0.8410627638394668\n",
      "Epoch: 2/2 Step : 2100 Train Loss : 0.833539 Validation error 0.8405309411040995\n",
      "Epoch: 2/2 Step : 2200 Train Loss : 0.873966 Validation error 0.8424297313906531\n",
      "Epoch: 2/2 Step : 2300 Train Loss : 0.835302 Validation error 0.8440596762242893\n",
      "Epoch: 2/2 Step : 2400 Train Loss : 0.835678 Validation error 0.8439827012109435\n",
      "Epoch: 2/2 Step : 2500 Train Loss : 0.767274 Validation error 0.8443283420410133\n",
      "Epoch: 2/2 Step : 2600 Train Loss : 0.896396 Validation error 0.8567275873617772\n",
      "Epoch: 2/2 Step : 2700 Train Loss : 0.782550 Validation error 0.8481073770911909\n",
      "Epoch: 2/2 Step : 2800 Train Loss : 0.914955 Validation error 0.8440400397463271\n",
      "Epoch: 2/2 Step : 2900 Train Loss : 0.603844 Validation error 0.84179465610655\n",
      "Epoch: 2/2 Step : 3000 Train Loss : 0.598354 Validation error 0.8425625136920384\n",
      "Epoch: 2/2 Step : 3100 Train Loss : 0.697069 Validation error 0.8394424385230721\n",
      "Epoch: 2/2 Step : 3200 Train Loss : 0.744486 Validation error 0.8379994129052855\n",
      "Epoch: 2/2 Step : 3300 Train Loss : 0.748554 Validation error 0.8403895875926694\n",
      "Epoch: 2/2 Step : 3400 Train Loss : 0.894946 Validation error 0.8349923220497929\n",
      "Epoch: 2/2 Step : 3500 Train Loss : 0.689940 Validation error 0.8334317725907798\n",
      "Epoch: 2/2 Step : 3600 Train Loss : 0.817644 Validation error 0.8359123290973818\n",
      "Epoch: 2/2 Step : 3700 Train Loss : 0.707501 Validation error 0.8400627878419757\n",
      "Epoch: 2/2 Step : 3800 Train Loss : 0.787894 Validation error 0.8413655858219656\n",
      "Epoch: 2/2 Step : 3900 Train Loss : 0.905752 Validation error 0.8409151107639276\n",
      "Epoch: 2/2 Step : 4000 Train Loss : 0.671841 Validation error 0.8329590344487487\n",
      "Epoch: 2/2 Step : 4100 Train Loss : 0.569635 Validation error 0.8312049205138009\n",
      "Epoch: 2/2 Step : 4200 Train Loss : 0.672068 Validation error 0.8312969683212126\n",
      "Epoch: 2/2 Step : 4300 Train Loss : 0.954272 Validation error 0.8352127661009203\n",
      "Epoch: 2/2 Step : 4400 Train Loss : 0.720645 Validation error 0.8312037148627673\n",
      "Epoch: 2/2 Step : 4500 Train Loss : 0.702531 Validation error 0.8315576099677001\n",
      "Epoch: 2/2 Step : 4600 Train Loss : 0.862841 Validation error 0.8315854514580721\n",
      "Epoch: 2/2 Step : 4700 Train Loss : 0.737767 Validation error 0.8291780062060938\n",
      "Epoch: 2/2 Step : 4800 Train Loss : 0.851724 Validation error 0.8323825233410941\n",
      "Epoch: 2/2 Step : 4900 Train Loss : 0.696144 Validation error 0.8282295176399624\n",
      "Epoch: 2/2 Step : 5000 Train Loss : 0.620620 Validation error 0.8410439904228739\n",
      "Epoch: 2/2 Step : 5100 Train Loss : 0.824561 Validation error 0.8333413052566208\n",
      "Epoch: 2/2 Step : 5200 Train Loss : 0.804981 Validation error 0.8330932891602753\n",
      "Epoch: 2/2 Step : 5300 Train Loss : 0.742543 Validation error 0.8277102495029176\n",
      "Epoch: 2/2 Step : 5400 Train Loss : 0.740074 Validation error 0.8296975648198517\n",
      "Epoch: 2/2 Step : 5500 Train Loss : 0.619176 Validation error 0.8293510316742628\n",
      "Epoch: 2/2 Step : 5600 Train Loss : 0.742874 Validation error 0.8300962980395807\n",
      "Epoch: 2/2 Step : 5700 Train Loss : 0.737681 Validation error 0.8281792679642842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2 Step : 5800 Train Loss : 0.753665 Validation error 0.8344398609777669\n",
      "Epoch: 2/2 Step : 5900 Train Loss : 0.811841 Validation error 0.829122519251907\n",
      "Epoch: 2/2 Step : 6000 Train Loss : 0.689154 Validation error 0.8285814414610533\n",
      "Epoch: 2/2 Step : 6100 Train Loss : 0.840450 Validation error 0.8306769025537736\n",
      "Epoch: 2/2 Step : 6200 Train Loss : 0.808474 Validation error 0.8255580418282386\n",
      "Epoch: 2/2 Step : 6300 Train Loss : 0.831370 Validation error 0.8323390120234978\n",
      "Epoch: 2/2 Step : 6400 Train Loss : 0.718893 Validation error 0.8287927971925566\n",
      "Epoch: 2/2 Step : 6500 Train Loss : 0.571792 Validation error 0.8303051192397942\n",
      "Epoch: 2/2 Step : 6600 Train Loss : 0.645140 Validation error 0.8300679635775009\n",
      "Epoch: 2/2 Step : 6700 Train Loss : 0.588844 Validation error 0.8280074976105986\n",
      "Epoch: 2/2 Step : 6800 Train Loss : 0.737817 Validation error 0.8327551814656109\n",
      "Epoch: 2/2 Step : 6900 Train Loss : 0.665697 Validation error 0.8290881757572485\n",
      "Epoch: 2/2 Step : 7000 Train Loss : 0.638401 Validation error 0.8266230161426844\n",
      "Epoch: 2/2 Step : 7100 Train Loss : 0.756251 Validation error 0.8276794625892733\n",
      "Epoch: 2/2 Step : 7200 Train Loss : 0.909272 Validation error 0.8237164016152948\n",
      "Epoch: 2/2 Step : 7300 Train Loss : 0.544182 Validation error 0.8283757244166533\n",
      "Epoch: 2/2 Step : 7400 Train Loss : 0.699677 Validation error 0.8235223252739664\n",
      "Epoch: 2/2 Step : 7500 Train Loss : 0.730649 Validation error 0.8234226436866419\n",
      "Epoch: 2/2 Step : 7600 Train Loss : 0.745981 Validation error 0.8183992211618429\n",
      "Epoch: 2/2 Step : 7700 Train Loss : 0.922198 Validation error 0.8220467224835911\n",
      "Epoch: 2/2 Step : 7800 Train Loss : 0.755036 Validation error 0.824255142418027\n",
      "Epoch: 2/2 Step : 7900 Train Loss : 0.701685 Validation error 0.8288905329677972\n",
      "Epoch: 2/2 Step : 8000 Train Loss : 0.914732 Validation error 0.8264724479943678\n",
      "Epoch: 2/2 Step : 8100 Train Loss : 0.617086 Validation error 0.8211291406433835\n",
      "Epoch: 2/2 Step : 8200 Train Loss : 0.681400 Validation error 0.8240970891359611\n",
      "Epoch: 2/2 Step : 8300 Train Loss : 0.669780 Validation error 0.8176005844467626\n",
      "Epoch: 2/2 Step : 8400 Train Loss : 0.569385 Validation error 0.8208110435545481\n",
      "Epoch: 2/2 Step : 8500 Train Loss : 0.433816 Validation error 0.8226098783684245\n",
      "Epoch: 2/2 Step : 8600 Train Loss : 0.808712 Validation error 0.8184525395448335\n",
      "Epoch: 2/2 Step : 8700 Train Loss : 0.863124 Validation error 0.8164776532398565\n",
      "Epoch: 2/2 Step : 8800 Train Loss : 0.523695 Validation error 0.8241449837886359\n",
      "Epoch: 2/2 Step : 8900 Train Loss : 0.792031 Validation error 0.8220034910812472\n",
      "Epoch: 2/2 Step : 9000 Train Loss : 0.776051 Validation error 0.8164929280355618\n",
      "Epoch: 2/2 Step : 9100 Train Loss : 0.676019 Validation error 0.8175943894374598\n",
      "Epoch: 2/2 Step : 9200 Train Loss : 0.776812 Validation error 0.8210709727957963\n",
      "Epoch: 2/2 Step : 9300 Train Loss : 0.728205 Validation error 0.8125595811098942\n",
      "Epoch: 2/2 Step : 9400 Train Loss : 0.841394 Validation error 0.8174884920102697\n",
      "Epoch: 2/2 Step : 9500 Train Loss : 0.888994 Validation error 0.815242546383842\n",
      "Epoch: 2/2 Step : 9600 Train Loss : 0.802951 Validation error 0.8165728513957677\n",
      "Epoch: 2/2 Step : 9700 Train Loss : 0.747203 Validation error 0.8191692378716846\n",
      "Epoch: 2/2 Step : 9800 Train Loss : 0.698277 Validation error 0.8158756240169962\n",
      "Epoch: 2/2 Step : 9900 Train Loss : 0.854615 Validation error 0.8137321212516834\n",
      "Epoch: 2/2 Step : 10000 Train Loss : 0.865517 Validation error 0.8201097882576035\n",
      "Epoch: 2/2 Step : 10100 Train Loss : 0.859278 Validation error 0.81528846315633\n",
      "Epoch: 2/2 Step : 10200 Train Loss : 0.824808 Validation error 0.8154818723349393\n",
      "Epoch: 2/2 Step : 10300 Train Loss : 0.715343 Validation error 0.8161558250500921\n",
      "Epoch: 2/2 Step : 10400 Train Loss : 0.766944 Validation error 0.8116338366675713\n",
      "Epoch: 2/2 Step : 10500 Train Loss : 0.808479 Validation error 0.8095118615760312\n",
      "Epoch: 2/2 Step : 10600 Train Loss : 0.804721 Validation error 0.8080698351491674\n",
      "Epoch: 2/2 Step : 10700 Train Loss : 0.694473 Validation error 0.8113591395441847\n",
      "Epoch: 2/2 Step : 10800 Train Loss : 0.792943 Validation error 0.8174137447159543\n",
      "Epoch: 2/2 Step : 10900 Train Loss : 0.655107 Validation error 0.8086721364849554\n",
      "Epoch: 2/2 Step : 11000 Train Loss : 0.636728 Validation error 0.8082542539117966\n",
      "Epoch: 2/2 Step : 11100 Train Loss : 0.535612 Validation error 0.8105279166847559\n",
      "Epoch: 2/2 Step : 11200 Train Loss : 0.766800 Validation error 0.8079151688069379\n",
      "Epoch: 2/2 Step : 11300 Train Loss : 0.689396 Validation error 0.8099892846212148\n",
      "Epoch: 2/2 Step : 11400 Train Loss : 0.715142 Validation error 0.8086956409791143\n",
      "Epoch: 2/2 Step : 11500 Train Loss : 0.657422 Validation error 0.8105603730400818\n",
      "Epoch: 2/2 Step : 11600 Train Loss : 0.608097 Validation error 0.8094194586440729\n",
      "Epoch: 2/2 Step : 11700 Train Loss : 0.759087 Validation error 0.808776546262803\n",
      "Epoch: 2/2 Step : 11800 Train Loss : 0.837738 Validation error 0.8037807900871988\n",
      "Epoch: 2/2 Step : 11900 Train Loss : 0.780675 Validation error 0.8070395729207028\n",
      "Epoch: 2/2 Step : 12000 Train Loss : 0.865007 Validation error 0.8151736652865723\n",
      "Epoch: 2/2 Step : 12100 Train Loss : 0.733090 Validation error 0.8100915582431452\n",
      "Epoch: 2/2 Step : 12200 Train Loss : 0.795452 Validation error 0.8087284351732744\n",
      "Epoch: 2/2 Step : 12300 Train Loss : 0.760687 Validation error 0.8062409977105993\n",
      "Epoch: 2/2 Step : 12400 Train Loss : 0.668993 Validation error 0.8051453588894728\n",
      "Epoch: 2/2 Step : 12500 Train Loss : 1.007186 Validation error 0.8048432858205149\n",
      "Epoch: 2/2 Step : 12600 Train Loss : 0.715535 Validation error 0.8063051084811376\n",
      "Epoch: 2/2 Step : 12700 Train Loss : 0.786698 Validation error 0.8061157543187051\n",
      "Epoch: 2/2 Step : 12800 Train Loss : 0.786972 Validation error 0.8050455006443599\n",
      "Epoch: 2/2 Step : 12900 Train Loss : 0.791274 Validation error 0.8030565376482004\n",
      "Epoch: 2/2 Step : 13000 Train Loss : 0.833284 Validation error 0.8048004238317525\n",
      "Epoch: 2/2 Step : 13100 Train Loss : 0.612810 Validation error 0.8030366814078034\n",
      "Epoch: 2/2 Step : 13200 Train Loss : 0.815693 Validation error 0.8043711118578253\n",
      "Epoch: 2/2 Step : 13300 Train Loss : 0.803113 Validation error 0.8052167283801634\n",
      "Epoch: 2/2 Step : 13400 Train Loss : 0.666526 Validation error 0.8048954309860232\n",
      "Epoch: 2/2 Step : 13500 Train Loss : 0.594443 Validation error 0.8028486946073505\n",
      "Epoch: 2/2 Step : 13600 Train Loss : 0.824767 Validation error 0.8053241316377568\n",
      "Epoch: 2/2 Step : 13700 Train Loss : 0.801370 Validation error 0.80056593690015\n",
      "Epoch: 2/2 Step : 13800 Train Loss : 0.527272 Validation error 0.800557975386053\n",
      "Epoch: 2/2 Step : 13900 Train Loss : 0.555023 Validation error 0.8051667918572756\n",
      "Epoch: 2/2 Step : 14000 Train Loss : 0.833036 Validation error 0.8087761915037048\n",
      "Epoch: 2/2 Step : 14100 Train Loss : 0.657780 Validation error 0.807218694946358\n",
      "Epoch: 2/2 Step : 14200 Train Loss : 0.683857 Validation error 0.8044271424714222\n",
      "Epoch: 2/2 Step : 14300 Train Loss : 0.858899 Validation error 0.8032550526717774\n",
      "Epoch: 2/2 Step : 14400 Train Loss : 0.603168 Validation error 0.8018959036347034\n",
      "Epoch: 2/2 Step : 14500 Train Loss : 0.785049 Validation error 0.797827637118843\n",
      "Epoch: 2/2 Step : 14600 Train Loss : 0.592231 Validation error 0.8010509436953513\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train your model with dropout. Make sure to clip your gradients.\n",
    "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "\n",
    "print_every = 100\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Starting epoch {}'.format(epoch + 1))\n",
    "    \n",
    "    steps = 0\n",
    "    for text_batch, labels in dataloader(\n",
    "            train_features, train_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        \n",
    "        hidden = model.init_hidden(text_batch.size(1))\n",
    "        \n",
    "        # Set Device\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "        \n",
    "        # TODO Implement: Train Model\n",
    "        model.zero_grad()\n",
    "        \n",
    "        out,hidden = model(text_batch, hidden)\n",
    "        \n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_loss_list = []\n",
    "            # TODO Implement: Print metrics\n",
    "            for valid_text_batch, labels in dataloader(\n",
    "                valid_features,  valid_labels, batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "                \n",
    "                valid_text_batch, labels = valid_text_batch.to(device), labels.to(device)\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "                \n",
    "                # print(valid_text_batch.size())\n",
    "                # valid_text_batch : [] 제일 마지막 남은게 다 쓰이지 못하고 \n",
    "                # torch.Size([20, 64]) , torch.Size([20, 16])\n",
    "                hidden = model.init_hidden(valid_text_batch.size(1)) #[layer, batch, lstm_size] = [2,64,512]\n",
    "                # Expected hidden[0] size (2, 16, 512), got (2, 64, 512)\n",
    "                out, hidden = model(valid_text_batch, hidden)\n",
    "                val_loss = criterion(out, labels)\n",
    "                val_loss_list.append(val_loss.item())\n",
    "                \n",
    "            print('Epoch: {}/{}'.format(epoch+1, epochs),\n",
    "                  'Step : {}'.format(steps),\n",
    "                  'Train Loss : {:.6f}'.format(loss.item()),\n",
    "                  'Validation error {}'.format(np.mean(val_loss_list)))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path='model'):\n",
    "    \"\"\"\n",
    "    Save the model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    checkpoint = {'vocab_size': model.vocab_size,\n",
    "                  'embed_size': model.embed_size,\n",
    "                  'lstm_size': model.lstm_size,\n",
    "                  'output_size' : model.output_size,\n",
    "                  'lstm_layers' : model.lstm_layers,\n",
    "                  'dropout' : model.dropout,\n",
    "                  'state_dict': model.state_dict() }\n",
    "    with open(path, 'wb') as f:\n",
    "        torch.save(checkpoint, f)\n",
    "    \n",
    "def load_model(path='model'):\n",
    "    \"\"\"\n",
    "    Load the model parameters\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        checkpoint = torch.load(f)\n",
    "        \n",
    "    model = TextClassifier(checkpoint['vocab_size'], checkpoint['embed_size'], checkpoint['lstm_size'],\n",
    "                  checkpoint['output_size'], checkpoint['lstm_layers'], checkpoint['dropout'])\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Save and Load Test\"\"\"\n",
    "save_model(model, 'my_model')\n",
    "# print(model)\n",
    "# next(model.parameters()).data\n",
    "\n",
    "# t = load_model('my_model')\n",
    "# next(t.parameters()).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "### Prediction \n",
    "Okay, now that you have a trained model, try it on some new twits and see if it works appropriately. Remember that for any new text, you'll need to preprocess it first before passing it to the network. Implement the `predict` function to generate the prediction vector from a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab):\n",
    "    \"\"\" \n",
    "    Make a prediction on a single sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : The string to make a prediction on.\n",
    "        model : The model to use for making the prediction.\n",
    "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred : Prediction vector\n",
    "    \"\"\"    \n",
    "    \n",
    "    # TODO Implement\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    \n",
    "    # Filter non-vocab words\n",
    "    tokens = [word for word in tokens if word in vocab]\n",
    "    # Convert words to ids\n",
    "    tokens = [vocab[word] for word in tokens]\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = torch.tensor(tokens).unsqueeze(1)\n",
    "    # Get the NN output\n",
    "    hidden = model.init_hidden(1)\n",
    "    logps, _ = model(text_input, hidden)\n",
    "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
    "    pred = torch.exp(logps)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3600e-04, 3.9721e-02, 2.1482e-02, 8.6741e-01, 7.0752e-02]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: What is the prediction of the model? What is the uncertainty of the prediction?\n",
    "\n",
    "\n",
    "The model predict the sentiment probability distribution given input text. With the given text, My model output the prediction vector of <6.3600e-04, 3.9721e-02, 2.1482e-02, 8.6741e-01, 7.0752e-02>. Since the highest probability corresponds to the positive sentiment, The prediction for the text is positive sentiment. And the uncertainty is (1 - 8.6741e-01)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
    "\n",
    "## Testing\n",
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
    "with open('test_twits.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
       " 'timestamp': '2018-11-01T00:00:05Z'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data']:\n",
    "        yield twit\n",
    "\n",
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `prediction` function, let's apply it to a stream of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_twits(stream, model, vocab, universe):\n",
    "    \"\"\" \n",
    "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
    "    \"\"\"\n",
    "    for twit in stream:\n",
    "\n",
    "        # Get the message text\n",
    "        text = twit['message_body']\n",
    "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
    "        score = predict(text, model, vocab)\n",
    "\n",
    "        for symbol in symbols:\n",
    "            if symbol in universe:\n",
    "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': '$AAPL',\n",
       " 'score': tensor([[0.1305, 0.0518, 0.0862, 0.1313, 0.6003]], grad_fn=<ExpBackward>),\n",
       " 'timestamp': '2018-11-01T00:00:18Z'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
    "\n",
    "next(score_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You have successfully built a model for sentiment analysis! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
